# General Assembly Data Science Class 
6/14/2016 to 8/18/2016

**Instructor: Hamed Hasheminia**


Tuesdays | Thursdays
--- | ---
6/14: Data Science - Introduction Part I | 6/16 Data Science - Introduction Part II
6/21: Linear Regression Lines Part I | 6/23: Linear Regression Lines Part II 
6/28: Model Selection | 6/30: Missing Data and Imputation
7/5: K-Nearest Neighbors | 7/7: Logistic Regression Part I
7/12: Logistic Regression Part II | 7/14: In Class Project 
7/19: Tree-Based Models Part I | 7/21: Tree-Based Models Part II
7/26: Natural Language Processing | 7/28: Time Series Models
8/2: Principal Component Analysis | 8/4: Data Visualization
8/9: Naive Bayes | 8/11: Course Review
8/16: Final Project Presentations I | 8/18: Final Project Presentations II




##Lecture 1 Summary (Data Science - Introduction Part I)


 - Data Science - meaning
 - Continuous, Discrete and Qualitative Data
 - Supervised vs Unsupervised Learning
 - Classification vs Regression
 - Time series vs cross-sectional data
 - Numpy
 - Pandas
 
**Resources**
 
 - [Lecture 1 - Introduction - Slides](https://github.com/ga-students/DS-SF-24/blob/master/Lecture%20Notes-Slides/Lecture%201%20-%20Introduction.pptx)
 - [Intro Numpy - Code](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture%201_Intro_Numpy.ipynb)
 - [Intro Numpy - Code - Solutions](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture%201_Intro_Numpy_Solutions.ipynb)
 - [Intro Pandas - Code](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture%201_Intro_Pandas.ipynb)
 - [InClass Practice Code - Pandas](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture%201-Practice-Code.ipynb)
 - [InClass Practice Code - Solutions](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture%201-Practice-Code-Solutions.ipynb)
 
**Set up GitHub - Self-study guide**

 - [Lecture 0 - GitHub - Slides](https://github.com/ga-students/DS-SF-24/blob/master/Lecture%20Notes-Slides/Lecture%200%20-%20GitHub%20-%20Self-study.pptx) 
 - [excellent videos to set-up github](https://www.youtube.com/playlist?list=PL5-da3qGB5IBLMp7LtN8Nc3Efd4hJq0kD). Students who have not used GitHub before must watch these videos.
 - A hands-on introduction to Git and GitHub, and how to make them work together! More Git resources for beginners [here](http://www.dataschool.io/tag/git/)
 
**Pre-work for second lecture**
 
 - Review all lecture notes including [Lecture Slides](https://github.com/ga-students/DS-SF-24/blob/master/Lecture%20Notes-Slides/Lecture%201%20-%20Introduction.pptx), [Numpy notebook](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture1_Intro_Numpy.ipynb), and [Pandas notebook](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture%201-Practice-Code.ipynb)
 - Finish self-study Github guidlines listed above
 - **Finish** [Inclass Practice Code](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture%201-Practice-Code.ipynb)
 - Review [final project requirements](https://github.com/ga-students/DS-SF-24/blob/master/final-projects/readme.md). You can find timelines for final project at slide 11 of [Lecture 1](https://github.com/ga-students/DS-SF-24/blob/master/Lecture%20Notes-Slides/Lecture%201%20-%20Introduction.pptx) PowerPoint Slides
  
**Additional Resources**

 - [Official Pandas Tutorials](http://pandas.pydata.org/pandas-docs/stable/tutorials.html). Wes & Company's selection of tutorials and lectures
 - [Julia Evans Pandas Cookbook](https://github.com/jvns/pandas-cookbook). Great resource with examples from weather, bikes and 311 calls
 - [Learn Pandas Tutorials](https://bitbucket.org/hrojas/learn-pandas). A great series of Pandas tutorials from Dave Rojas
 - [Research Computing Python Data PYNBs](https://github.com/ResearchComputing/Meetup-Fall-2013/tree/master/python). A super awesome set of python notebooks from a meetup-based course exclusively devoted to pandas
 
 
## Lecture 2 Summary (Data Science Intorduction - Part II)
- Measures of central tendency (Mean, Median, Mode, Quartiles, Percentiles)- Measures of Variability (IQR, Standard Deviation, Variance)- Skewness Coefficient - Boxplots, Histograms, Scatterplots- Central Limit Theorem- Class/Dummy Variables
- Walkthrough describing and visualizing data in Pandas

**Resources**
 
 - [Lecture 2 - Slides](https://github.com/ga-students/DS-SF-24/blob/master/Lecture%20Notes-Slides/Lecture%202%20-%20Introduction%20-%20Part%202.pptx) 
 - [Basic Statistics - Part 2 - Lab Codes](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture%202%20-%20Intro.ipynb) 
 - [Basic Statistics - Part 2 - Practice Code](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture%202-%20Intro%20-Practice-Code.ipynb)
 - [Basic Statistics - Part 2 - Practice Code - Solutions](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture%202-%20Intro%20-Practice-Code-Solutions.ipynb)


**HW 1 is Assigned**

 - Please read and follow instructions from [readme](https://github.com/ga-students/DS-SF-24/blob/master/HW%20Assignments/HW1/readme.md)
 - This homework is due on June 23rd, 2016 at 6:30PM

**Additional Resources**

 - [Here](http://matplotlib.org/resources/index.html) you can find valuable resources for matplotlib
 - [A good Video](https://www.youtube.com/watch?v=Pujol1yC1_A) on Centeral Limit Theorem
 

## Lecture 3 Summary (Linear Regression Lines - Part I)
- Linear Regression lines
- Single Variable and Multi-Variable Regression Lines
- Capture non-linearity using Linear Regression lines.
- Interpretting regression coefficients
- Dealing with dummy variables in regression lines
- intro on sklearn and searborn library

**Resources**
 
- [Lecture 3 - Slides](https://github.com/ga-students/DS-SF-24/blob/master/Lecture%20Notes-Slides/Lecture%203%20-%20Linear%20Regression%20-%20Part%201.pptx) 
- [Linear Regression - Part I - Lab Codes](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture%203%20-%20Linear%20Regression%20-%20Part%201.ipynb) 
- [Linear Regression - Part I - Practice Code](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture%203-Practice-Code.ipynb)
- [Linear Regression - Part I - Practice Solutions](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture%203-Practice-Solution.ipynb)

**Additional Resources**

- My videos on regression lines. [Video 1](https://www.youtube.com/watch?v=QRzaKZRqens), [Video 2](https://www.youtube.com/watch?v=F6ceTd56vc0)
- This is an [excellent book](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Sixth%20Printing.pdf). In Lecture 3 and Lecture 4, we are going to cover Chapter 3 of this textbook.
- [Seaborn](https://web.stanford.edu/~mwaskom/software/seaborn/api.html) 
- [Weighted Least Square Method (WLS)](http://www.econ.uiuc.edu/~wsosa/econ471/GLSHeteroskedasticity.pdf) 
- Good resource for [heteroskedasticity](http://www.statsmakemecry.com/smmctheblog/confusing-stats-terms-explained-heteroscedasticity-heteroske.html)
- [Here](http://math.arizona.edu/~calc/Text/Section12.3.pdf) Contours are elegantly introduced.)


## Lecture 4 Summary (Linear Regression Lines - Part II)
- Hypothesis test - test of significance on regression coefficients
- p-values
- Capture non-linearity using Linear Regression lines.
- R-squared
- Interaction Effects

**Resources**
 
- [Lecture 4 - Slides](https://github.com/ga-students/DS-SF-24/blob/master/Lecture%20Notes-Slides/Lecture%204%20-%20Regression%20-%20Part%202.pptx) 
- [Linear Regression - Part II - Lab Codes](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture%204%20-%20Linear%20Regression%20-%20Part%202.ipynb) 
- [Linear Regression - Part II - Practice Code](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture%204-Practice-Code.ipynb)
- [Linear Regression - Part II - Practice Solution](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture4-Practice-Solution.ipynb)

**Additional Resources**

- My videos on regression lines. [Video 1](https://www.youtube.com/watch?v=QRzaKZRqens), [Video 2](https://www.youtube.com/watch?v=F6ceTd56vc0)
- This is an [excellent book](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Sixth%20Printing.pdf). In Lecture 3 and Lecture 4, we covered Chapter 3 of this textbook.
- [statmodels.formula.api](http://statsmodels.sourceforge.net/devel/)

**HW 2 is Assigned**

 - Please read and follow instructions from [readme](https://github.com/ga-students/DS-SF-24/blob/master/HW%20Assignments/HW2/readme.md)
 - [Here](https://github.com/ga-students/DS-SF-24/blob/master/HW%20Assignments/HW2/HW2-Starter-Code.ipynb) you can find iPython notebook of your 2nd assignment.
 - This homework is due on June 30th, 2016 at 6:30PM

## Lecture 5 Summary (Model Selection)

- Bias-Variance Trade off
- Validation (Test vs Train set)
- Cross-Validation
- Ridge and Lasso Regression
- (Optional) Backward Selection, Forward Selection, All Subset Selection. (If you want to use these methods you need to use R)

**Resources**
 
 - [Lecture 5 - Slides](https://github.com/ga-students/DS-SF-24/blob/master/Lecture%20Notes-Slides/Lecture%205%20-%20Model%20Selection.pptx) 
 - [Model Selection - Lab Codes](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture5-Model_Selection.ipynb) 
 - [Model Selection  - Practice Code](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture5-Practice-Code.ipynb) 
 - [Model Selection - Practice Solutions](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture5-Practice-Solution.ipynb)
 - [HW 1 - Key](https://github.com/ga-students/DS-SF-24/blob/master/HW%20Assignments/HW1/HW1-Key.ipynb)
 
 
**Additional Resources**
 
 - [Preprocessing](http://scikit-learn.org/stable/modules/preprocessing.html) Library
 - [Cross-Validation](http://scikit-learn.org/stable/modules/cross_validation.html) Library
 - This is an [excellent book](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Sixth%20Printing.pdf). You can find theory of Cross-Validation in Chapter 5. You can also learn about Lasso and Ridge regression in Chapter 6 of the mentioend textbook.
 - [Here](https://www.youtube.com/watch?v=oTeXrcyaqKA) you can find my video on Cross-Validation
 - [Here](https://www.youtube.com/watch?v=2KTRa3QKvMY) you can find my video on Ridge and Lasso Regression
 - [Here](https://www.youtube.com/watch?v=fV1LQV0bQTU) you can find my video on Best subset selection.
 
## Lecture 6 Summary (Missing Data and Imputation)

- Types of missing data (MCAR, MAR, NMAR)
- Single imputation and their limitations
- Imuptation using regression lines and error
- Hot deck imputation
- multiple imputation

**Resources**
 
- [Lecture 6 - Slides](https://github.com/ga-students/DS-SF-24/blob/master/Lecture%20Notes-Slides/Lecture%206-Missing%20Data-Imputation.pptx) 
- [Missing Data and Imputation - Lab Codes](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture6-MissingData.ipynb) 
- [Missing Data and Imputation  - Practice Code and HW 3](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture6-Practice-Code.ipynb) 
- [Missing Data and Imputation - Solution Code](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture6-Practice-Solution.ipynb)

**Additional Resources**

- Great [Video](https://www.youtube.com/watch?v=xnQ17bbSeEk) by Dr. Elizabeth A. Stuart from John Hopkins University

**Announcements**

- **HW 3 is assigned** (Due at 6:30PM - July 7th)
- Please read [this](https://github.com/ga-students/DS-SF-24/blob/master/HW%20Assignments/HW3/readme.md) before starting your assignment.
- HW3 starter code can be found [here](https://github.com/ga-students/DS-SF-24/blob/master/HW%20Assignments/HW3/HW3-Starter-code.ipynb)

## Lecture 7 Summary (K-Nearest Neighbors)

- Classification Problems
- Misclassifciation Error
- KNN algorithm for Classification
- Cross-Validation for KNN Algorithm
- Limitations of KNN Algorithm
- KNN algorithm for Regression

**Resources**
 
- [Lecture 7 - Slides](https://github.com/ga-students/DS-SF-24/blob/master/Lecture%20Notes-Slides/Lecture%207%20-%20KNN.pptx) 
- [K-Nearest Neighbors - Lab Codes](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture7-KNN.ipynb) 
- [K-Nearest Neighbors  - Practice Code](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture7-Practice-Code.ipynb) 
- [K-Nearest Neighbors - Practice Solution](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture7-Practice-Solution.ipynb)

**Announcements**

- [HW 2 Solutions](https://github.com/ga-students/DS-SF-24/blob/master/HW%20Assignments/HW2/HW2-Solution.ipynb) are posted.

## Lecture 8 Summary (Logistic Regression Part I)

- Logistic Regression - Intro
- Odds vs Probability
- Using Logistic Regression to Make predictions
- How one interprets coefficients of a Logistic Regression model
- Strength and weaknesses of Logistic Regression Model

**Resources**
 
- [Lecture 8 - Slides](https://github.com/ga-students/DS-SF-24/blob/master/Lecture%20Notes-Slides/Lecture%208-Logistic_Regression_Part%20I.pptx) 
- [Logistic Regression Part I - Lab Codes](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture8.ipynb) 
- [Logistic Regression Part I - Practice Code](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture8-Practice-Code.ipynb) 
- [Logistic Regression Part I - Practice Solutions](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture8-Practice-Solutions.ipynb)

**Additional Resources**

- Logistic Regression [video](https://www.youtube.com/watch?time_continue=374&v=r-yv6GbWep4)

**HW 3 Solutions Posted**

- [HW 3 Soltions](https://github.com/ga-students/DS-SF-24/blob/master/HW%20Assignments/HW3/HW3-Solution.ipynb)

## Lecture 9 Summary (Logistic Regression Part II)

- Unbalanced observations and Logistic Regression
- FP/FN/TP/TN/FPR/TPR
- The effect of changing Threshhold
- ROC Curves
- Area Under Curve
- How to compare classifciation algorithms

**Resources**
 
- [Lecture 9 - Slides](https://github.com/ga-students/DS-SF-24/blob/master/Lecture%20Notes-Slides/Lecture%209-Logistic%20Regression_Part%20II.pptx)
- [Logistic Regression Part II - Lab Codes](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture9.ipynb) 
- [Logistic Regression Part II - Practice Code](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture9-Practice-Code.ipynb) 
- [Logistic Regression Part II - Practice - Solutions](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture9-Practice-Solution.ipynb)

## Lecture 10 (In-Class Projects)


- [Breast Cancer Project](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture10_Project_Breast_Cancer.ipynb)
- [Breast Cancer - Group Notebook](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture10_Project_Breast_Cancer_Group_Try.ipynb)
- [Energy Efficiency Project](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture10_Project_Energy_Efficiency.ipynb)
- [Energy Efficiency - Group Notebook](https://github.com/ga-students/DS-SF-24/blob/master/Code/lecture10_project_energy_efficiency_Group_Try.ipynb)
- [Income Prediction Project](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture10_Project_Income_Prediction.ipynb)
- [Wine Quality Project](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture10_Project_White_Wine_quality.ipynb)
- [Wine Quality - Group Notebook](https://github.com/ga-students/DS-SF-24/blob/master/Code/lecture10_project_energy_efficiency_Group_Try.ipynb)

## Lecture 11 Summary (Tree-Based Models - part I)

- Decision Tree for Regression
- Greedy Approach
- Decision Tree for Classification
- Gini Index and Entropy index
- Limitation of Simple Decision Trees

**Resources**
 
- [Lecture 11 - Slides](https://github.com/ga-students/DS-SF-24/blob/master/Lecture%20Notes-Slides/Lecture%2011%20-%20Tree-Based%20Models%20-%20Part%20I.pptx)
- [Decision Trees Part I - Lab Codes](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture11.ipynb) 
- [Decision Trees Part I - Practice Code](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture11-Practice-Code.ipynb) 

**Additional Resources**

- [Tree-Based Models - Video 1](https://www.youtube.com/watch?v=U-dYqlvafYA)
- [Tree-Based Models - Video 2](https://www.youtube.com/watch?v=6fopQt_ENeU)
- If you are among the ones who hate dealing with dummy variables, enjoy working with this [dummify function](https://github.com/ga-students/DS-SF-24/blob/master/Resources%20for%20Students/Dummify_Function.ipynb)


## Lecture 12 Summary Tree-Based Models - part II)

- Bagging
- Random Forest
- Boosting
- Tuning parameters for boosting and Random Forest

**Resources**
 
- [Lecture 12 - Slides](https://github.com/ga-students/DS-SF-24/blob/master/Lecture%20Notes-Slides/Lecture%2012%20-%20Tree-Based%20Models%20Part%20II.pptx)
- [Decision Trees Part II - Lab Codes](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture12.ipynb) 
- [Decision Trees Part II - Practice Code](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture12-Practice-Code.ipynb) 



**Additional Resources**


- [Bagging, Boosting, and Random Forest - Video](https://www.youtube.com/watch?v=BaPmPEWxKu0)
- [BootStrap - Video](https://www.youtube.com/watch?v=8bLsk1WXgDk)

**Announcement**

- [HW 4](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture12-Practice-Code.ipynb) is assigned and is due on July 28th 2016 at 6:30PM.
- Please read [ReadMe](https://github.com/ga-students/DS-SF-24/blob/master/HW%20Assignments/HW4/readme.md) file before working on your project. 

## Lecture 13 Summary (Natural Language Processing)
- Definition of Natural Language Processing 
- NLP applications
- Basic NLP practice
- Stop words, bag-of-words, TF-DIF

**Resources**
 
- [Lecture 13 - Slides](https://github.com/ga-students/DS-SF-24/blob/master/Lecture%20Notes-Slides/Lecture%2013%20-%20NLP.pptx)
- [Natural Language Processing - Lab Codes](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture13.ipynb) 
- [Natural Language Processing - Practice Code](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture13-Practice-Code.ipynb) 
- [Natural Language Processing - Practice Solutions](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture13-Practice-Solution.ipynb)

**Additional Resources**

* If you want to learn a lot more NLP, check out the excellent [video lectures](https://class.coursera.org/nlp/lecture) and [slides](http://web.stanford.edu/~jurafsky/NLPCourseraSlides.html) from this [Coursera course](https://www.coursera.org/course/nlp) (which is no longer being offered).
* [Natural Language Processing with Python](http://www.nltk.org/book/) is the most popular book for going in-depth with the [Natural Language Toolkit](http://www.nltk.org/) (NLTK).
* [A Smattering of NLP in Python](https://github.com/charlieg/A-Smattering-of-NLP-in-Python/blob/master/A%20Smattering%20of%20NLP%20in%20Python.ipynb) provides a nice overview of NLTK, as does this [notebook from DAT5](https://github.com/justmarkham/DAT5/blob/master/notebooks/14_nlp.ipynb).
* [spaCy](http://spacy.io/) is a newer Python library for text processing that is focused on performance (unlike NLTK).
* If you want to get serious about NLP, [Stanford CoreNLP](http://nlp.stanford.edu/software/corenlp.shtml) is a suite of tools (written in Java) that is highly regarded.
* When working with a large text corpus in scikit-learn, [HashingVectorizer](http://scikit-learn.org/stable/modules/feature_extraction.html#vectorizing-a-large-text-corpus-with-the-hashing-trick) is a useful alternative to CountVectorizer.
* [Automatically Categorizing Yelp Businesses](http://engineeringblog.yelp.com/2015/09/automatically-categorizing-yelp-businesses.html) discusses how Yelp uses NLP and scikit-learn to solve the problem of uncategorized businesses.
* [Modern Methods for Sentiment Analysis](http://districtdatalabs.silvrback.com/modern-methods-for-sentiment-analysis) shows how "word vectors" can be used for more accurate sentiment analysis.
* [Identifying Humorous Cartoon Captions](http://www.cs.huji.ac.il/~dshahaf/pHumor.pdf) is a readable paper about identifying funny captions submitted to the New Yorker Caption Contest.

**Pre-Work**

- [Bayes Examples](https://github.com/ga-students/DS-SF-24/blob/master/Resources%20for%20Students/Bayes-Examples.pdf)
- [Bayes Example - Solutions](https://github.com/ga-students/DS-SF-24/blob/master/Resources%20for%20Students/BayesExamples-Solutions.pdf)
- [Principal Component Analysis Video](https://www.youtube.com/watch?v=IPuWD5QfpkA)

## Lecture 14 Summary (Principal Component Analysis)

- Principal Component Analysis
- Computation of PCAs
- Geometry of PCAs
- Proportion of Variance Explained

**Resources**
 
- [Lecture 14 - Slides](https://github.com/ga-students/DS-SF-24/blob/master/Lecture%20Notes-Slides/Lecture%2014%20-%20PCA.pptx)
- [Principal Component Analysis - Lab Codes](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture14.ipynb) 
- [Principal Component Analysis - Practice Code](https://github.com/ga-students/DS-SF-24/blob/master/Code/Lecture14-Practice-Code.ipynb) 

**Additional Resources**

* [This tutorial](http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf) on Principal Components Analysis (PCA) includes good refreshers on covariance and linear algebra
* To go deeper on Singular Value Decomposition, read [Kirk Baker's excellent tutorial](https://www.ling.ohio-state.edu/%7Ekbaker/pubs/Singular_Value_Decomposition_Tutorial.pdf).
* Chapter 10 of  [Statistical Learning with applications in R](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Fourth%20Printing.pdf)
